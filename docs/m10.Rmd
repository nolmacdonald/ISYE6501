# Module 10 - Advanced Regression

Module 10 covers advanced topics in regression, 
including tree-based approaches that can be used more generally.

## M10L1 - Introduction to CART

*Lecture 1 - Introduction to Cart (5:57)*

### Overview

- **Previously:**
  - Building, Fitting, Evaluating, and Using Regression Models
- **This Lesson:**
  - Using Trees
    - Dividing a data set
    - Different models for different subsets

### Trees in Regression

- Classification Problems
  - **CART:** Classification and Regression Trees
- Decision Making
  - Decision Tree

**Single Regression Modeling:**
  - Ex: Impact of a marketing email on recipient spending
  - Predictors
    - Demographics: age, sex, number of children, income, etc.
    - Purchasing Factors: avg. amount spent per month on web site
    - Binary Factor: Was the email received

```{r, fig.show='hold', fig.cap="Regression for Money Spent 25 Years Old or Younger", out.width="100%", fig.align='center', echo=FALSE}
knitr::include_graphics("figures/m10-trees-in-regression.png")
```

- Regression for "25 years old or younger"

\begin{equation}
  \mathrm{Money \, Spent} = \\ 50 +
  13.75[\mathrm{Number \, of \, Children}] +
  0[\mathrm{Income \, over \,} \$30,000] + \dots
\end{equation}

- Regression for "Older than 25"

\begin{equation}
  \mathrm{Money \, Spent} = \\ 
  32 + 28.13[\mathrm{Number \, of \, Children}] +
  7.13[\mathrm{Income \, over \,} \$30,000] + \dots
\end{equation}

```{r, fig.show='hold', fig.cap="Trees in Regression with Leaves", out.width="50%", fig.align='center', echo=FALSE}
knitr::include_graphics("figures/m10-regression-tree-leaves.png")
```

- Fewer data points per node
- Lots of regressions

```{r, fig.show='hold', fig.cap="Regression Tree", out.width="70%", fig.align='center', echo=FALSE}
knitr::include_graphics("figures/m10-regression-tree.png")
```

### Regression Formula in Trees

- Full Regression: **NOT FOR TREES**
  - $y = a_0 + a_1x_1 + a_2x_2 + \dots + a_nx_n$


**Regression in Trees:** $y = a_0$

\begin{equation}
  a_0 = \frac{\sum_i \mathrm{in \, node \,} y_i}{\mathrm{Number \, of \, data \, points \, in \, node}}
\end{equation}

- **Quick Derivation of Best Fit:**
  - $\underset{a_0}{\min} \sum_i (y_i-a_o)^2$
- **Set Derivative to Zero:**
  - $2 \sum_i (y_i - a_0) = 0$
  - $\sum_i y_i = k a_0$
  - $a_0 = \frac{\sum_i y_i}{k}$

### Final Comments

- Other places to use trees
  - Logistic regression models
    - Fraction of node's data points with "true" response
  - Classification models
    - Most common classification among node's data points
  - Decision models
    - Ex: Each leaf is the decision to "Do I send a marketing email?"

**Common Questions:**

- How do we choose what branches to put into a tree?
- When do we stop branching?
- Why is this method called a Regression "Tree"?

## M10L2 - Branching

*Lecture 2 - Branching (3:42)*

### Overview

- **Previously:**
  - What is tree-based analysis
  - Why should we consider making different models for different data subsets
- **This Lesson:**
  - A tree's branches
    - Specifying
    - Stopping conditions

### Branching

### Branching Methods

### Summary

- How to perform branching
- When should a branch be kept in the model

## M10L3 - Random Forests

*Lecture 3 - Random Forests (3:58)*

### Overview

- **Previously:**
  - What is tree-based analysis
  - Why should we consider making different models for different data subsets
- **This Lesson:**
  - Random Forest Method

### Tree Branchind

### Random Forests

### Introducing Randomness

### Random Forests

### Summary

## M10L3A - Explainability/Interpretability

*Lecture 3A - Explainability/Interpretability (5:36)*

### Overview

## M10L4 - Logistic Regression

*Lecture 4 - Logistic Regression (6:30)*

### Overview

### Overview

## M10L5 - Confusion Matrices

*Lecture 5 - Confusion Matrices (5:05)*

### Overview

## M10L6 - Situationally-Driven Comparison

*Lecture 6 - Situationally-Driven Comparison (3:46)*

### Overview

## M10L7 - Advanced Topics in Regression

*Lecture 7 - Advanced Topics in Regression (5:02)*

### Overview

