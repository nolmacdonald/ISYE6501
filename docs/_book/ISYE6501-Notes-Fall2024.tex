% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Ensure text is aligned to the left margin
\usepackage{geometry}
\geometry{
  left=1in,    % Adjust these margins as necessary
  right=1in,
  top=1in,
  bottom=1in,
  bindingoffset=0in
}

% Disable paragraph indentation
\setlength{\parindent}{0pt}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Course Notes - Introduction to Analytics Modeling},
  pdfauthor={Nolan MacDonald},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Course Notes - Introduction to Analytics Modeling}
\author{Nolan MacDonald}
\date{Fall 2024}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Test}\label{test}

\chapter{Module 1 - Introduction}\label{module-1---introduction}

\section{M1L1 - Course Overview}\label{m1l1---course-overview}

\section{M1L2 - Course Structure}\label{m1l2---course-structure}

\section{M1L2a - Homework Grading and Q\&A}\label{m1l2a---homework-grading-and-qa}

\section{M1L3 - Modeling}\label{m1l3---modeling}

\chapter{Module 2 - Classification}\label{module-2---classification}

\section{M2L1 - Introduction to Classification}\label{m2l1---introduction-to-classification}

\section{M2L2 - Choosing a Classifier}\label{m2l2---choosing-a-classifier}

\section{M2L3 - Data Definitions}\label{m2l3---data-definitions}

\section{M2L4 - Support Vector Machines (SVMs)}\label{m2l4---support-vector-machines-svms}

\section{M2L5 - SVM: What the Name Means}\label{m2l5---svm-what-the-name-means}

\section{M2L6 - Advanced Support Vector Machines}\label{m2l6---advanced-support-vector-machines}

\section{M2L7 - Scaling and Standardization}\label{m2l7---scaling-and-standardization}

\section{M2L8 - K-Nearest-Neighbor (KNN) Algorithm}\label{m2l8---k-nearest-neighbor-knn-algorithm}

\chapter{Module 3 - Validation}\label{module-3---validation}

\section{Overview}\label{overview}

Module 3 will continue with basic machine learning algorithms.
The modules will cover couple of cross-cutting concepts and the important topic of model validation.

\section{M3L1 - Introduction to Validation}\label{m3l1---introduction-to-validation}

Validation

\begin{itemize}
\tightlist
\item
  How good is the model?
\end{itemize}

Data has two types of patterns

\begin{itemize}
\tightlist
\item
  \textbf{Real Effect} - Real relationship between attributes and response
\item
  \textbf{Random Effect} - Random, but looks like a real effect
\end{itemize}

Fitting matches both real and random effects

\begin{itemize}
\tightlist
\item
  Real effects - Same in all data sets
\item
  Random effects - Different in all data sets
\end{itemize}

\textbf{Example: What day of the month were you born?}

\begin{itemize}
\tightlist
\item
  Training Data: 3, 21, 24, 24, 25, 26, 27, 30, 30, 31
\item
  Best Predictor: You were born on the 26th

  \begin{itemize}
  \tightlist
  \item
    Right in the middle of 9/10 data points
  \end{itemize}
\item
  \textbf{This is a random effect!}
\item
  This model using 9/10 from 21-31 doesn't have a large error
\item
  If new data showed 2, 9, 11, 12, 14, 21, 24, 24, 29, 31

  \begin{itemize}
  \tightlist
  \item
    Much larger error due to the uniform spread over the month
  \end{itemize}
\item
  Was this just luck? (3, 21, 24, etc.)

  \begin{itemize}
  \tightlist
  \item
    No, some random pattern would have shown up

    \begin{itemize}
    \tightlist
    \item
      Early in month
    \item
      Middle of month
    \item
      Even/odd numbered day
    \item
      Day is multiple of 3
    \item
      Day is close to one of my kids birthdays
    \item
      Etc.
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{M3L1 - Summary}

\begin{itemize}
\tightlist
\item
  The example proves we can't measure the model's effectiveness on data it was trained on
\item
  Model fit captures real and random effects
\item
  Only real effects are duplicated in other data
\end{itemize}

\emph{Don't judge a model based on how well it fits the training data}.

Key Points on Validation

\begin{itemize}
\tightlist
\item
  Validation is crucial to determine how good a model is and how accurately it performs on new data
\item
  Measuring a model's performance on the same training data used to create it is not a good approach, as it will be too optimistic
\item
  Any dataset contains both real effects (true relationships) and random effects (patterns that occur by chance)
\item
  When fitting a model to training data, it captures both real and random effects
\item
  However, when using the model on new data, only the real effects will persist, while the random effects will be different
\item
  An example is given of a silly model that predicts people's birth dates based on a random pattern in the training data, which would not generalize well
\item
  The key takeaway is that we cannot rely on training data performance to evaluate a model - we need a separate validation process to get an accurate assessment of its effectiveness
\end{itemize}

\section{M3L2 - Validation and Test Data Sets}\label{m3l2---validation-and-test-data-sets}

\section{M3L3 - Splitting Data}\label{m3l3---splitting-data}

\section{M3L4 - Cross-Validation}\label{m3l4---cross-validation}

\chapter{Module 4 - Clustering}\label{module-4---clustering}

\section{Overview}\label{overview-1}

Module 4 will continue with basic machine learning algorithms.
The focus will be on clustering models.
The modules will cover couple of cross-cutting concepts, including distance norms, and k-means clustering.

\section{M4L1 - Introduction to Clustering}\label{m4l1---introduction-to-clustering}

\section{M4L2 - Distance Norms}\label{m4l2---distance-norms}

\section{M4L3 - K-Means Clustering}\label{m4l3---k-means-clustering}

\section{M4L4 - Practical Details for K-Means}\label{m4l4---practical-details-for-k-means}

\chapter{Appendix A: Glossary}\label{appendix-a-glossary}

\section{Basic Machine Learning}\label{basic-machine-learning}

\emph{Lessons 2.1-2.2, 2.4-2.6, 2.8, 4.1, 4.3-4.6, 6.1-6.3, 16.4}

\textbf{Algorithm}: Step-by-step procedure designed to carry out a task.

\textbf{Change detection}: Identifying when a significant change has taken place in a process.

\textbf{Classification}: The separation of data into two or more categories, or (a point's classification) the category a data point is put into.

\textbf{Classifier}: A boundary that separates the data into two or more categories. Also (more generally) an algorithm that performs classification.

\textbf{Cluster}: A group of points identified as near/similar to each other.

\textbf{Cluster center}: In some clustering algorithms (like ùëò-means clustering), the central point (often the centroid) of a cluster of data points.

\textbf{Clustering}: Separation of data points into groups (``clusters'') based on nearness/similarity to each other. A common form of unsupervised learning.

\textbf{CUSUM}: Change detection method that compares observed distribution mean with a threshold level of change. Short for ``cumulative sum''.

\textbf{Deep learning}: Neural network-type model with many hidden layers.

\textbf{Dimension}: A feature of the data points (for example, height or credit score). (Note that there is also a mathematical definition for this word.)

\textbf{EM algorithm}: Expectation-maximization algorithm.

\textbf{Expectation-maximization algorithm (EM algorithm)}: General description of an algorithm with two steps (often iterated), one that finds the function for the expected likelihood of getting the response given current parameters, and one that finds new parameter values to maximize that probability.

\textbf{Heuristic}: Algorithm that is not guaranteed to find the absolute best (optimal) solution.

\newpage

\textbf{k-means algorithm}: Clustering algorithm that defines ùëò clusters of data points, each corresponding to one of ùëò cluster centers selected by the algorithm.

\textbf{k-Nearest-Neighbor (KNN)}: Classification algorithm that defines a data point's category as a function of the nearest ùëò data points to it.

\textbf{Kernel}: A type of function that computes the similarity between two inputs; thanks to what's (really!) sometimes known as the ``kernel trick'', nonlinear classifiers can be found almost as easily as linear ones.

\textbf{Learning}: Finding/discovering patterns (or rules) in data, often that can be applied to new data.

\textbf{Machine}: Apparatus that can do something; in ``machine learning'', it often refers to both an algorithm and the computer it's run on. (Fun fact: before computers were developed, the term ``computers'' referred to people who did calculations quickly in their heads or on paper!)

\textbf{Margin}: For a single point, the distance between the point and the classification boundary; for a set of points, the minimum distance between a point in the set and the classification boundary. Also called the separation.

\textbf{Machine learning}: Use of computer algorithms to learn and discover patterns or structure in data, without being programmed specifically for them.

\textbf{Misclassified}: Put into the wrong category by a classifier.

\textbf{Neural network}: A machine learning model that itself is modeled after the workings of neurons in the brain.

\textbf{Supervised learning}: Machine learning where the ``correct'' answer is known for each data point in the training set.

\textbf{Support vector}: In SVM models, the closest point to the classifier, among those in a category. (Note that there is a more-technical mathematical definition too.)

\textbf{Support vector machine (SVM)}: Classification algorithm that uses a boundary to separate the data into two or more categories (``classes'').

\textbf{SVM}: Support vector machine.

\textbf{Unsupervised learning}: Machine learning where the ``correct'' answer is not known for the data points in the training set.

\textbf{Voronoi diagram}: Graphical representation of splitting a plane with two or more special points into regions with one special point each, where each region's points are closest to that special point.

\end{document}
