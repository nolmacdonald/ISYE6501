[["index.html", "Course Notes - Introduction to Analytics Modeling 1 Table of Contents", " Course Notes - Introduction to Analytics Modeling Nolan MacDonald Fall 2024 1 Table of Contents Module 1: Introduction Module 2: Classification Module 3: Validation Module 4: Clustering Module 5: Basic Data Preparation Module 6: Change Detection Module 7: Time Series Models Module 8: Basics of Regression Module 9: Data Preparation and PCA Module 10: Advanced Regression "],["module-1---introduction.html", "2 Module 1 - Introduction 2.1 M1L1 - Course Overview 2.2 M1L2 - Course Structure 2.3 M1L2a - Homework Grading and Q&amp;A 2.4 M1L3 - Modeling", " 2 Module 1 - Introduction 2.1 M1L1 - Course Overview 2.2 M1L2 - Course Structure 2.3 M1L2a - Homework Grading and Q&amp;A 2.3.1 Homework Format The main focus of the homework will be your analysis of your results, not your code. For a top score, you shouldn’t just run some code and display some results; rather, you should also discuss the results qualitatively, point out anything surprising, and comment on possible explanations. The top recommended forms for your analysis, in order, are: pdf, html, txt, and then word doc. We recommend not including your name / e-mail in your homework PDF. We (TAs) and the system will know who you are, and submitting anonymously will not cause any problems assigning your grade later. It will also avoid potential problems of bias. 2.3.2 Grading Homeworks Once you’ve submitted your homework, you will have to complete 3-Peer Reviews. You are given a rubric with scoring guidelines with possible values of 100, 90, 75, 50, and 0. When doing the grading, Canvas does allow you to give options outside of the scale, but please stick to it. I can see who gave the off-scale score, and it only frustrates your fellow students and makes my job a little harder. Also, if you have a homework marked as “late” in your grading queue, please grade it as normal. We have a very small tolerance between when peer reviews are assigned and homeworks are due to allow for students who had technical issues to submit. 100 is “All correct (perhaps except a few details) with a deeper solution than expected” This means that if students gave answers that delved deeper into the analytical methods and modeling techniques with appropriate solutions and explanations etc, then that constitutes a 100. All questions are completed. When giving this grade, please comment what you believe the person did well and how they went above and beyond. I cannot force you to leave a comment, but it will help stay consistent with the regrading policy that will be mentioned later. 90 is for “most or all correct.” This means that student provided code, answered the questions asked with code output, and provided reasonable but basic explanations for their answers. There can be minor errors like slight inconsistencies or minor misunderstandings. All questions are answered. This is the “default” grade. If a student does what is asked, and provides reasonable explanation for their work, but doesn’t go above and beyond, this is their grade. While comments here can be nice, if you have nothing else to add other than “they did everything right,” you do not have to leave a comment. 75 is “not correct, but a reasonable attempt.” There is some code, solutions, and explanation, but the explanation is faulty, incorrect, or non-existent or the solution values are completely different than outlined in the homework solutions or solutions and explanation do not make sense. At least half of the questions are answered/attempted including the coding questions. When giving this grade, please comment what you believe the person did that was fundamentally incorrect. This will help the student learn and will also help me in case of a regrade request. 50 is “Not correct, insufficient effort.” There is a distinct lack of effort on homework like no code, no answers, no explanations, or little of any of these. Particularly, if a student only answers a descriptive problem (one which involves answering a prompt) and does none of the coding problems, then this would also be considered insufficient effort. When giving this grade, please comment what you believe the person did that was fundamentally incorrect and explain what part of the homework they did not do. This will help the student learn and will also help me in case of a regrade request. 0 is “Not Submitted” There is nothing of merit submitted. The student did not attempt the problems, just submitted random or unrelated work to try and obtain a 50. When giving this grade, it is because the student literally had nothing there, was completely unrelated (a picture of a ham-sandwich), or basically un-attempted. If you feel like the student was just being lazy, you most likely should give them a zero. Other info: Optional truly means optional. Doing an optional part does not guarantee a 100. On the other end, You can still get a 100 even if you don’t do an optional part. An optional part can contribute to a deeper analysis, but isn’t required nor does it guarantee it. Please try to be helpful with your comments if you provide any. Also, try to provide appropriate feedback or comments based on the homework questions being asked. For example, I could say that “exploring more C values for Q2.2.1 would have shown a wider range/change in the model accuracy” or “using different kernels would result in higher model accuracy” These comments are appropriate but do not result in a drop in the student’s grade since exploring different kernels was optional and we did not give a specific range of C values to look at. 2.4 M1L3 - Modeling "],["module-2---classification.html", "3 Module 2 - Classification 3.1 M2L1 - Introduction to Classification 3.2 M2L2 - Choosing a Classifier 3.3 M2L3 - Data Definitions 3.4 M2L4 - Support Vector Machines (SVMs) 3.5 M2L5 - SVM: What the Name Means 3.6 M2L6 - Advanced Support Vector Machines 3.7 M2L7 - Scaling and Standardization 3.8 M2L8 - K-Nearest-Neighbor (KNN) Algorithm", " 3 Module 2 - Classification 3.1 M2L1 - Introduction to Classification 3.2 M2L2 - Choosing a Classifier 3.3 M2L3 - Data Definitions 3.4 M2L4 - Support Vector Machines (SVMs) 3.5 M2L5 - SVM: What the Name Means 3.6 M2L6 - Advanced Support Vector Machines 3.7 M2L7 - Scaling and Standardization 3.8 M2L8 - K-Nearest-Neighbor (KNN) Algorithm "],["module-3---validation.html", "4 Module 3 - Validation 4.1 Overview 4.2 M3L1 - Introduction to Validation 4.3 M3L2 - Validation and Test Data Sets 4.4 M3L3 - Splitting Data 4.5 M3L4 - Cross-Validations", " 4 Module 3 - Validation 4.1 Overview Module 3 will continue with basic machine learning algorithms. The modules will cover couple of cross-cutting concepts and the important topic of model validation. Additional References: [1]. A Survey of Cross-Validation Procedures for Model Selection 4.2 M3L1 - Introduction to Validation Validation How good is the model? Data has two types of patterns Real Effect - Real relationship between attributes and response Random Effect - Random, but looks like a real effect Fitting matches both real and random effects Real effects - Same in all data sets Random effects - Different in all data sets Example: What day of the month were you born? Training Data: 3, 21, 24, 24, 25, 26, 27, 30, 30, 31 Best Predictor: You were born on the 26th Right in the middle of 9/10 data points This is a random effect! This model using 9/10 from 21-31 doesn’t have a large error If new data showed 2, 9, 11, 12, 14, 21, 24, 24, 29, 31 Much larger error due to the uniform spread over the month Was this just luck? (3, 21, 24, etc.) No, some random pattern would have shown up Early in month Middle of month Even/odd numbered day Day is multiple of 3 Day is close to one of my kids birthdays Etc. 4.2.1 M3L1 - Summary The example proves we can’t measure the model’s effectiveness on data it was trained on Model fit captures real and random effects Only real effects are duplicated in other data Don’t judge a model based on how well it fits the training data. Validation is crucial to determine how good a model is and how accurately it performs on new data Measuring a model’s performance on the same training data used to create it is not a good approach, as it will be too optimistic Any dataset contains both real effects (true relationships) and random effects (patterns that occur by chance) When fitting a model to training data, it captures both real and random effects However, when using the model on new data, only the real effects will persist, while the random effects will be different An example is given of a silly model that predicts people’s birth dates based on a random pattern in the training data, which would not generalize well The key takeaway is that we cannot rely on training data performance to evaluate a model - we need a separate validation process to get an accurate assessment of its effectiveness 4.3 M3L2 - Validation and Test Data Sets Measure a model’s performance: A larger set of data to fit the model A smaller set of data to measure the model’s effectiveness Splitting Data: Training set (larger) to fit model Validation set (smaller) to estimate effectiveness Training and Validation Sets: Observed performance = real quality + random effects High-performing models more likely to have above-average random effects Observed performance of chosen model is probably too optimistic Test Sets: Training data set to fit the models Validation data set to choose best model Test data set to estimate performance of chosen model Overall: Training Set - Building models Validation Set - Picking a model Test Set - Estimating performance of chosen model 4.3.1 Summary Using only training data to evaluate a model’s performance is often too optimistic, as the model overfits to random patterns in the training data. To get a better measure of performance, we use separate validation and test sets: Training set: Used to fit/build the model Validation set: Used to evaluate and compare different models Test set: Used to get an unbiased final estimate of the chosen model’s performance The validation set helps select the best model, but its performance estimate may still be inflated due to random chance when choosing the “best” model. The test set provides a final unbiased performance estimate for the selected model. General process: Train multiple models on training data Evaluate models on validation set and select best one Estimate final performance of chosen model on test set This three-way split helps avoid overfitting and provides a more realistic assessment of how well the model will generalize to new data. There are different ways to split data into training, validation and test sets, which will be covered in a future lesson. The key takeaway is that using separate datasets for training, model selection, and final evaluation helps produce more reliable and generalizable machine learning models. 4.4 M3L3 - Splitting Data Training data too optimistic so we need to use a test set Training data set to build model Validation set to compare models Test set to estimate performance of chosen model How do we split data into traning, validation, and test sets? Method 1: Random - Randomly choose data points Method 2: Rotation - Take turns selecting points Training-Validation-Training-Test-Training Randomness could give one set more early or late data Rotation equally separates data Rotation may introduce bias 4.4.1 Summary Data needs to be split into training, validation, and test sets to properly evaluate model performance. Recommended splits: For two sets: 70-90% training, 10-30% testing (Rule of thumb) For three sets: 50% training, 25% validation, 25% testing 60% training, 20% validation, 20% testing 70% training, 15% validation, 15% testing Two main approaches for splitting data: Simple randomness: Randomly assign data points to each set Rotation: Systematically rotate through assigning points to each set Advantages of rotation: Ensures equal representation of data across time periods Avoids potential biases from random sampling Disadvantages of rotation: May introduce other biases if not done carefully (e.g. only certain days of week in each set) A hybrid approach combining randomness and rotation can be used to avoid biases. Cross-validation is another technique for using data, which will be covered in a future lesson. It is important to properly split data to get accurate model evaluations, while there are tradeoffs between different splitting approaches. 4.5 M3L4 - Cross-Validations What if important data only appears in validation or test sets? Use cross-validation! Use of k in analytics k-means k-nearest neighbor k-fold cross validation k-fold Cross-Validation For each of the k parts: Train the model on all the other parts Evaluate it on the one remaining part Average the k evaluations to estimate the model’s quality No standard number for k, but k=10 is common Figure 4.1: k-Fold Cross Validation ANSWER: NONE! Do not average the coefficients over four splits Train the model again using all the data k-Fold Cross-Validation provides: Better use of data Better estimate of model quality Choose model more effectively 4.5.1 Summary Purpose of Cross-Validation: Cross-validation is introduced as a technique to ensure that important data points are not excluded from the training set, which can happen if they only appear in the validation or test sets. This method helps in making better use of the data available. Types of Cross-Validation: The lecture specifically discusses k-fold cross-validation, a popular method in analytics. The ‘k’ in k-fold cross-validation indicates the number of parts the data is split into for training and validation purposes. Process of K-Fold Cross-Validation: The data is divided into k parts. For example, if k=4, the data is split into four parts. The model is trained on k-1 parts and validated on the remaining part. This process is repeated k times, with each part being used as the validation set once. Every data point is used for training in k-1 models, ensuring no important data is left out. Model Evaluation: The performance of the model is evaluated by averaging the results from the k different validation sets. This average provides an estimate of the model’s quality. Choosing the Final Model: After using cross-validation to select a model, the final model is retrained using all the data parts together. This ensures the model benefits from the full dataset. Common Practice: While there is no standard number for k, using k=10 is common in practice. Overall, cross-validation is emphasized as a crucial step in model selection and evaluation, helping to improve the reliability of the analytics process. References [1] S. Arlot and A. Celisse, “A survey of cross-validation procedures for model selection,” 2010. "],["module-4---clustering.html", "5 Module 4 - Clustering 5.1 Overview 5.2 M4L1 - Introduction to Clustering 5.3 M4L2 - Distance Norms 5.4 M4L3 - K-Means Clustering 5.5 M4L4 - Practical Details for K-Means 5.6 M4L5 - Clustering for Prediction 5.7 M4L6 - Clustering v. Classification", " 5 Module 4 - Clustering 5.1 Overview Module 4 will continue with basic machine learning algorithms. The focus will be on clustering models. The modules will cover couple of cross-cutting concepts, including distance norms, and k-means clustering. Additional References: [2]. Data Mining Algorithms In R/Clustering/K-Means 5.2 M4L1 - Introduction to Clustering Clustering: An unsupervised machine learning technique designed to group unlabeled examples based on their similarity to each other. Grouping data points Important to note: If the examples are labeled, the grouping is called classification Examples of Clustering: Targeted marketing/market segmentation Potential customers need a message that would be most likely to encourage them to buy For example, if we were selling a SUV: Size Price Versatility Coolness Each set of people would be a cluster We would try to use data to split consumers into sets to discover what marketing they should be shown You can examine a cluster and it may not always be correct, which can help you find a meaningful cluster in your data For example, we did not consider gas mileage Other examples: Targeted marketing/market segmentation Personalized medicine Locating facilities - Look at where people live and provide a police station for each cluster Image analysis - CAPCHA Initial data investigation 5.3 M4L2 - Distance Norms The choice of distance measures is important in clustering. Distance measures define how the similarity of the two element are calculated and influences the shape of the clusters. Euclidean (straight-line) distance: Distance in Euclidean space by length of straight line segment between two points \\[ distance = \\sqrt{\\sum^n_{i=1}(x_i - y_i)^2} = \\sqrt{(x_1-y_1)^2+(x_2-y_2)^2} \\] Rectilinear (Manhattan) Distance: Commonly used in city planning with a grid, hence Manhattan term \\[ distance = \\sum^n_{i=1} |x_i-y_i| = |x_1-y_1| + |x_2-y_2| \\] Minkowski (p-norm) Distance: We can describe both euclidean (p=2) and rectilinear (p=1) distance with p-norm (or Minkowski) distance \\[ distance = \\sqrt[p]{\\sum^n_{i=1} |x_i-y_i|^p} = \\sqrt[p]{|x_1-y_1|^p + |x_2-y_2|^p} \\] The most common values for p include 1, 2, and \\(\\infty\\) (\\(\\infty\\)-norm distance). Infinity-Norm Distance (\\(\\infty\\)-norm): The infinity norm simply measures how large the vector is by the magnitude of its largest entry. Simply put, it is the largest of a set of numbers in an absolute of values (the biggest). \\[ distance = \\lim_{p\\to\\infty} \\sqrt[\\infty]{\\sum^n_{i=1} |x_i-y_i|^{\\infty}} \\] The largest value for \\(\\infty\\) (assume p is 8) is dominated by the largest power. The term to the 7th power would be small compared to the 8th power. This means considering distance, the sum of terms is equal to the largest \\(|x_i-y_i|\\) to the infinity power. Note: Should include the limit to infinity, but for simplicity the equations do not all include the limit. \\[ distance = \\lim_{p\\to\\infty} \\sqrt[\\infty]{\\sum^n_{i=1} |x_i-y_i|^{\\infty}} = \\sqrt[\\infty]{\\max_i |x_i-y_i|^{\\infty}}=\\max_i |x_i-y_i| \\] 5.4 M4L3 - K-Means Clustering The K-Means algorithm is a popular technique of representative-based clustering. K-Means is a simple learning algorithm for clustering analysis. The goal of K-Means algorithm is to find the best division of n entities in k groups, so that the total distance between the group’s members and its corresponding centroid, representative of the group, is minimized [2]. Consider the K-Means algorithm defined as: \\[ \\min_{y,z} \\sum_i \\sum_k y_{ik} \\sqrt{\\sum_j (x_{ij}-z_{jk})^2} \\] The algorithm is subject to \\(\\sum_k y_{ik} = 1\\) for each i where: \\(x_{ij}\\) is attribute j of data point i \\(y_{ik}\\) is 1 if data point i is in cluster k, 0 if not \\(z_{jk}\\) is coordinate j of cluster center k Adds up all data points to cluster centers but only when the data point is in the cluster. Pick k cluster centers within range of data (e.g., Pick 3 points, k=3, where each point is a cluster center) Assign each data point to nearest cluster center Recalculate cluster centers (centroids of the data points in the cluster) This could result in new cluster centers with cluster points that are more applicable for another cluster. There is an iterative process for 1 and 2 that are repeated until there are no changes. Stops when no data points change clusters. K-Means Algorithm Overview: Machine Learning Heuristic - Fast, good, but not guaranteed to find absolute best solution Expectation-Maximization (EM) Algorithm Maximizing the negative distance to a cluster center 5.5 M4L4 - Practical Details for K-Means Using the K-Means algorithm in practice 5.5.1 Summary Handling Outliers: K-means will assign outliers to the nearest cluster, but this can distort results. While one option is to remove outliers, a more thoughtful approach is to investigate their significance and implications for your analysis. Algorithm Limitations: K-means is a heuristic, meaning it’s not guaranteed to find the best clustering but is efficient and often finds good solutions. To improve results, it’s advised to run k-means multiple times with different initial cluster centers and compare the outcomes. Determining the Number of Clusters: The number of clusters (k) can be optimized by running the algorithm with different k values and using an “elbow diagram” to identify where increasing the number of clusters no longer significantly improves the solution. However, practical considerations should also guide this choice, depending on the context. Balancing Science and Art: The lecture emphasizes the importance of blending data science with the “art” of analytics, where understanding the situation and making informed decisions can provide greater value than merely running algorithms. 5.6 M4L5 - Clustering for Prediction 5.6.1 Summary Clustering Recap: Clustering involves grouping data points based on their similarity and proximity. The k-means heuristic is a common method for finding good clusterings. Predictive Clustering: K-means clustering can be used predictively by determining which cluster a new data point should belong to, typically by finding the closest cluster center. Handling New Data Points: If a new data point falls within an existing cluster, it is straightforward to assign it to that cluster. If not, the point is assigned to the nearest cluster center. Voronoi Diagrams: The space around each cluster center can be divided into regions, where each region represents the area closer to that center than to any other. This is visualized using a Voronoi diagram. Historical Context: Voronoi diagrams have been used historically, including in the analysis of a cholera outbreak in London over 150 years ago, and by mathematicians like Rene Descartes in the 1600s. Old Ideas in Analytics: Some effective analytical techniques, like Voronoi diagrams, are not new but have been around for a long time and remain valuable. 5.7 M4L6 - Clustering v. Classification 5.7.1 Summary Classification Models: These involve a set of data points where both their attributes and correct groupings (responses) are known. For example, in loan application data, we know whether applicants repaid their loans (blue) or not (red). Classification models use both attributes and known responses to classify new data points. This process is known as supervised learning because it uses observed responses to guide the model. Clustering Models: In contrast, clustering models start with a set of data points where only the attributes are known, and the correct groupings are not known. The model must determine how to group the data points based solely on their attributes. This is known as unsupervised learning because there are no observed responses to guide the model. Supervised learning is more common in analytics (such as classification) but unsupervised learning (such as clustering) is also a valuable tool. References [2] Wikibooks, “Data mining algorithms in r/clustering/k-means.” http://en.wikipedia.org/w/index.php?title=K-means%20clustering&amp;oldid=1243054475, 2024. "],["module-5---basic-data-preparation.html", "6 Module 5 - Basic Data Preparation 6.1 Overview 6.2 M5L1 - Data Prep - Intro 6.3 M5L2 - Outlier Detection 6.4 M5L3 - Dealing with Outliers", " 6 Module 5 - Basic Data Preparation 6.1 Overview Reference: Outlier Analysis 6.2 M5L1 - Data Prep - Intro This lesson includes examples of model data and potential data issues. 6.2.1 Data Preparation Using specific data in our analysis - Predictors (regression), factors (classification), etc. Quantitative examples - Credit scores - Average daily temperature in Atlanta - Number of hot dogs sold - Price of stocks - Amount of certain chemmcial in drinking water Scale of the data - Household income ~10E5, Credit scores 10E2 6.3 M5L2 - Outlier Detection 6.4 M5L3 - Dealing with Outliers "],["module-6---change-detection.html", "7 Module 6 - Change Detection 7.1 Overview 7.2 M6L1 - Intro to Change Detection 7.3 M6L2 - Cumulative Sum (CUSUM) for Change Detection", " 7 Module 6 - Change Detection 7.1 Overview 7.2 M6L1 - Intro to Change Detection 7.3 M6L2 - Cumulative Sum (CUSUM) for Change Detection "],["module-7---time-series-models.html", "8 Module 7 - Time Series Models 8.1 M7L1 - Introduction to Exponential Smoothing 8.2 M7L2 - Trend and Cyclic Effects 8.3 M7L3 - Exponential Smoothing - What the Name Means 8.4 M7L4 - Forcasting 8.5 M7L5 - ARIMA 8.6 M7L6 - GARCH", " 8 Module 7 - Time Series Models 8.1 M7L1 - Introduction to Exponential Smoothing Lecture 1 - Introduction to Exponential Smoothing (3:58) 8.1.1 Time Series Data Time series data - Data where the same response is known for many time periods. Trends over time (e.g., stock price) Cyclical variations (e.g., temperature) Random variation (e.g., Temperatures, stock prices, blood pressure) 8.1.2 Exponential Smoothing Method Example: \\(S_t\\): Expected response or baseline (e.g., Blood pressure at hour t) \\(x_t\\): The observed response (e.g., Observed blood pressure at t) Two ways to answer: \\(S_t=x_t\\): Observed blood pressure is a real indicator of the baseline \\(S_t = S_{t-1}\\): Today’s baseline is the same as yesterdays baseline Solution - Exponential Smoothing Method: \\[ S_t = \\alpha x_t + (1 - \\alpha) S_{t-1} \\] In the equation above, \\(S_t\\) is the expected baseline response at time period \\(t\\). The observed response is defined as \\(x_t\\) and the range for \\(\\alpha\\) is defined as \\(0 &lt; \\alpha &lt; 1\\). Tuning \\(\\alpha\\) to 0 means there is a lot of randomness in the system, while a value of 1 means there is not a lot of randomness in the system. How to Start: Initial condition: \\(S_1 = x_1\\) Does not deal with trends or cyclical varations 8.2 M7L2 - Trend and Cyclic Effects Lecture 2 - Trend and Cyclic Effects (6:06) 8.2.1 Exponential Smoothing Method \\[ S_t = \\alpha x_t + (1 - \\alpha) S_{t-1} \\] \\(0 &lt; \\alpha &lt; 1\\) Helps trade off between trusting \\(x_t\\) when \\(\\alpha\\) is large Trusting \\(S_{t-1}\\) when \\(\\alpha\\) is small More randomness: Trust previous estimates \\(S_{t-1}\\), Small \\(\\alpha\\) Less randomness: Trust what you see, may indicate real change, larger \\(\\alpha\\) 8.2.2 Trends \\(T_t\\): the trend at time period t \\(S_t = \\alpha x_t + (1 - \\alpha) (S_{t-1} + T_{t-1})\\) Now includes the previous trend, \\(T_{t-1}\\) \\(T_{t} = \\beta (S_t - S_{t-1}) + (1-\\beta)T_{t-1}\\) \\(\\beta\\) times observed trend plus \\(\\beta\\) times previous trend Initial condition: \\(T_1=0\\) 8.2.3 Cylic Patterns Like trend - additive component of formula Alternative Seasonalities: Multiplicative way \\(L\\): Length of cycle \\(C_t\\): Multiplicative seasonality factor for time t Inflate or deflate the observation *New baseline formula (including trend and seasonality) \\[ S_t = \\frac{\\alpha x_t}{C_{t-L}} + (1-\\alpha)(S_{t-1}+T_{t-1}) \\] - Update the seasonal, or cyclic, factor in similar way - \\(C_t = \\gamma(x_t/S_t) + (1-\\gamma)C_{t-L}\\) - \\(C_1, \\dots, C_L=1\\) - No initial cyclic effect - \\(x_t\\): Observation - \\(S_t\\): Baseline - \\(C_{t-L}\\): Previous cylic factor Example: Selling burgers If C = 1.1 on Sunday, sales were 10% higher just because it was Sunday Of 550 sold on Sunday 500 = baseline value 50 = 10% extra Starting Conditions: Trend: \\(T_1=0\\) Shows no initial trend Multiplicative Seasonality: Multiplying by 1 Shows no initial cyclic effect First L values of C set to 1 8.2.4 Models Exponential smooth model Sometimes called single, double, triple Depends on how many seasonal aspects you include Triple exponential smooth is also called Winter’s method or Holt-Winters method 8.3 M7L3 - Exponential Smoothing - What the Name Means Lecture 3 - Exponential Smoothing - What the Name Means (3:35) Previously: Discussed exponential smoothing Analyzing time series data where the same response is known for many time periods 8.3.1 Basic single exponential smoothing \\[ S_t = \\alpha x_t + (1 - \\alpha) S_{t-1} \\] Example: \\(\\alpha=0.5\\) \\(S_t = 0.5 x_t + 0.5 S_{t-1}\\) High observed value \\(x_t\\): \\(S_t\\) not as high; pulled down by \\((1-\\alpha)S_{t-1}\\) Low observed value \\(x_t\\): \\(S_t\\) not as low; pulled up by \\((1-\\alpha)S_{t-1}\\) This is where the name exponential smoothing comes from! Peaks and valleys are smoothed out Figure 8.1: Exponential smoothing example \\(S_t = \\alpha x_t + (1 - \\alpha) S_{t-1}\\) \\(S_{t-1} = \\alpha x_{t-1} + (1-\\alpha)S_{t-2}\\) Plug in \\(S_{t-1}\\) and we now have: \\(S_t = \\alpha x_t + (1 - \\alpha) [\\alpha x_{t-1} + (1-\\alpha)S_{t-2}]\\) \\(= \\alpha x_t + (1 - \\alpha) \\alpha x_{t-1} + (1-\\alpha)^2 S_{t-2}\\) Where now \\(S_{t-2}=\\alpha x_t-2 + (1-\\alpha)S_{t-3}\\) Continuing we have an equation for \\(S_t\\) with observations from the past each weighted by \\(1-\\alpha\\) to an increasing exponent. Figure 8.2: Exponential equation expanded Every past observation contributes to the current baseline estimate More recent observations are more important Newer observations weighted more 8.4 M7L4 - Forcasting Lecture 4 - Forcasting (3:39) Exponential smoothing can be used for simple forecasting Predict what will happen next 8.4.1 Forecasting Remember the basic exponential smooth equation \\(S_t = \\alpha x_t + (1 - \\alpha) S_{t-1}\\) Prediction \\(S_{t+1} = \\alpha x_{t+1} + (1 - \\alpha) S_{t}\\) \\(x_{t+1}\\) is unknown Best guess: \\(x_{t+1}=S_t\\) Our forecast for time period t+1 \\(F_{t+1} = \\alpha S_t + (1-\\alpha) S_t\\) Our guess for next time period is same as current baseline \\(F_{t+1} = S_t\\) 8.4.2 Forecasting with Trend Include the trend \\(S_t = \\alpha x_t + (1 - \\alpha) (S_{t-1}+T_{t-1})\\) \\(T_{t} = \\beta (S_t - S_{t-1}) + (1-\\beta)T_{t-1}\\) The best estimate of the next baseline Most current baseline estimate The best estimate of the trend with the most current trend estimate Our forecast for time period t+1 \\(F_{t+1}=S_t + T_t\\) \\(F_{t+k}=S_t + k T_t, k=1,2,\\dots\\) 8.4.3 Forecasting with Multiplicative Seasonality Include multiplicative seasonality \\(S_t = \\frac{\\alpha x_t}{C_{t-L}} + (1-\\alpha)(S_{t-1}+T_{t-1})\\) The best estimate of the next time period’s seasonal factor $C_{t+1}=C_{(t+1)-L} Forecast for the time period t+1 \\(F_{t+1}=(S_t + T_t)C_{(t+1)-L}\\) \\(F_{t+k}=(S_t + kT_t)C_{(t+1)-L+(k-1)}, k=1,2,\\dots\\) 8.4.4 Future How to find good values for \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) Optimization: \\(\\min(F_t-x_t)^2\\) Other methods for analyzing time series data 8.5 M7L5 - ARIMA Lecture 5 - ARIMA (6:36) 8.5.1 ARIMA Auto Regressive Integrated Moving Average (ARIMA) - General method for measuring time-series data - No underlying theory discussed, in Time Series Analysis course ARIMA - 3 Key Parts 8.5.2 Part I - Differences Exponential Smoothing: \\(S_t = \\alpha x_t + (1 - \\alpha) S_{t-1}\\) \\(S_t = \\alpha x_t + (1 - \\alpha) \\alpha x_{t-1} + (1 - \\alpha)^2 \\alpha x_{t-2} + (1 - \\alpha)^3 \\alpha x_{t-3}+\\dots\\) Estimate \\(S_t\\) based on \\(x_t\\), \\(x_{t-1}\\), etc. Works well if data is stationary Stationary Process: If the mean, variance and other measures all expected to be constant over time Often data is not stationary, but the difference might be stationary For example: First order difference \\(D_{(1)}\\): Difference of consecutive observations \\(D_{(1)t}=(x_t - x_{t-1)}\\) Second order difference \\(D_{(2)}\\): Differences of the differences \\(D_{(2)t}=(x_t - x_{t-1)}-(x_{t-1}-x_{t-2})\\) Third order difference \\(D_{(3)}\\): Differences of the differences of the differences \\(D_{(3)t}=[(x_t - x_{t-1)}-(x_{t-1}-x_{t-2})]-[(x_{t-1}-x_{t-2})-(x_{t-2}-x_{t-3})]\\) 8.5.3 Part II - Autoregression Predicting the current value based on previous time periods’ values Regression: Predicting the value based on other factors Auto: Using earlier values to predict, only works with time series data Exponential smoothing: order-\\(\\infty\\) autogregressive model \\(S_t = \\alpha x_t + (1 - \\alpha) \\alpha x_{t-1} + (1 - \\alpha)^2 \\alpha x_{t-2} + (1 - \\alpha)^3 \\alpha x_{t-3}+\\dots\\) Uses data all the way back Order-p autoregressive model: \\(S_t\\) is a function of \\({x_t, x_{t-1}, x_{t-2}, \\dots, x_{t-(p-1)}}\\) Go back only p time periods ARIMA combines autoregression and differencing Autoregression on the differences Use p time periods of previous observations to predict d-th order differences 8.5.4 Part III - Moving Average Previous errors \\(\\epsilon_t\\) as predictors \\(\\epsilon_t = (\\hat{x}_t-x_t)\\) Order-q moving average Go back q time periods \\(\\epsilon_t, \\epsilon_{t-1}, \\epsilon_{t-2}, \\dots, \\epsilon_{t-q}\\) ARIMA(p,d,q) Model \\[ D_{(d)t}=\\mu + \\sum_{i=1}^p a_i D_{(d)t-i} -\\sum_{i=1}^q \\theta_i(\\hat{x}_{t-i}-x_{t-i}) \\] dth-order differences pth-order autoregression qth-order moving average General model Add seasonality Specific values of p, d, and q ARIMA(0,0,0): White noise ARIMA(0,1,0): Random walk ARIMA(p,0,0): AR (Autoregressive) model ARIMA(0,0,q): MA (Moving Average) model ARIMA(0,1,1): Basic exponential smoothing model Short-term forecasting Better than exponential smoothing When the data is more stable, with fewer peaks, valleys, and outliers Need 40 past data points for ARIMA to work well Summary: ARIMA models help estimate or forecast a value 8.6 M7L6 - GARCH Lecture 6 - GARCH (2:56) Generalized Autoregressive Conditional Heteroscedasticity (GARCH) - Estimate or forecast the variance 8.6.1 Variance Estimate the amount of error Example: Forecast demand for pickup trucks Variance - How much forecast might be higher or lower than true value 8.6.2 Variance Estimation Important in investment Traditional portfolio optimization model Balances the expected return of a set of investments with amount of volatility Types of portfolios Riskier: Higher expected return Less-risky: Lower expected return Variance: A Proxy for the amount of volatility or risk GARCH: A common approach for estimating variance ARIMA Model: \\[ D_{(d)t}=\\mu + \\sum_{i=1}^p a_i D_{(d)t-i} -\\sum_{i=1}^q \\theta_i(\\hat{x}_{t-i}-x_{t-i}) \\] GARCH Model: \\[ \\sigma_t^2 = \\omega + \\sum_{i=1}^p \\beta_i \\sigma_{t-i}^2 + \\sum_{i=1}^q \\gamma_i \\epsilon^2_{t-i} \\] Two differences from ARIMA: Variances/squared errors Not observations/linear errors Raw variances Not differences of variances 8.6.3 Summary Three methods - Analyzing time series Exponential smoothing ARIMA GARCH "],["module-8---basics-of-regression.html", "9 Module 8 - Basics of Regression 9.1 M8L1 - Introduction to Regression 9.2 M8L2 - Maximum Likelihood and Information Criteria 9.3 AIC Example 9.4 M8L3 - Using Regression 9.5 M8L4 - Causation vs. Correlation 9.6 M8L5 - Transformations and Interactions 9.7 M8L6 - Output", " 9 Module 8 - Basics of Regression Module 8 will cover the basics of regression, along with the concepts of estimating a model’s quality and distinguishing between correlation and causation. 9.1 M8L1 - Introduction to Regression Lecture 1 - Introduction to Regression (5:12) 9.1.1 Regression Regression is one of the most common models in analytics. Regression can answer two types of questions. How do systems work? Examples: Value of a home run Effect of economic factors on presidential election Impact of education on income Key factors in car purchasing What will happen in the future? Examples: Height of a child at adulthood Oil price 1.5 years from now Housing demand in next 6 months Remaining lifetime of insurance applicant 9.1.2 Simple Linear Regression (SLR) Linear regression with one predictor Look for linear relationship between predictor and response Example: Effect of unemployment on new car sales \\(y\\): Response (new car sales) \\(x_1\\): Predictor (workforce participation) Regression Equation: \\[\\begin{equation} y = a_0 + a_1x_1 \\tag{9.1} \\end{equation}\\] Regression Equation with \\(m\\) Predictors: \\[\\begin{equation} \\begin{split} y &amp;= a_0 + a_1x_1 + a_2x_2 + \\dots + + a_mx_m \\\\ &amp;= a_0 + \\sum_{j=1}^m a_jx_j \\end{split} \\tag{9.2} \\end{equation}\\] Figure 9.1: Linear regression with one predictor Measure the quality of the line’s fit with squared errors. \\(y_i\\): Cars sold for data point \\(i\\) \\(\\hat{y}_i\\): Model’s prediction of cars sold Data point \\(i\\) Prediction Error: \\[\\begin{equation} y_i - \\hat{y}_i = y_i - (a_0 + a_1x_{i1}) \\tag{9.3} \\end{equation}\\] Sum of Squared Errors: \\[\\begin{equation} \\begin{split} &amp; \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\ &amp;= \\sum_{i=1}^n (y_i - \\big(a_0 + a_1x_{i1})\\big)^2 \\end{split} \\tag{9.4} \\end{equation}\\] Figure 9.2: Linear regression prediction error Why the sum of squared errors? See lesson on estimating model quality Best-fit regression line Determine line fit by sum of squared errors Minimizes sum of squared errors Defined by \\(a_0\\) and \\(a_1\\) Underlying math Sum of squared errors - Minimize convex quadratic function Set partial derivatives to zero Solve simultaneous equations 9.1.3 Summary Linear regression Answers two questions How do systems work? What will happen in the future Key Equations: Regression Equation: Eq. (9.1) Regression Equation with \\(m\\) Predictors: Eq. (9.2) Data point \\(i\\) Prediction Error: Eq. (9.3) Sum of Squared Errors: Eq. (9.4) 9.2 M8L2 - Maximum Likelihood and Information Criteria Lecture 2 - Maximum Likelihood and Information Criteria (7:56) I this lesson, we cover how to measure the quality of a model’s fit. 9.2.1 Likelihood Likelihood: Basic measure of quality is likelihood Assume the data is the correct value and we have information about the variance Measure the probability (density) for any parameter set Maximum likelihood (highest probability density) Parameters that give the highest probability 9.2.2 Maximum Likelihood Example Example: Error ~ $N(0, ^2), i.i.d. i.i.d. - Independent and identically distributed random variables Observations: \\(z_1,\\dots,z_n\\) Model estimates: \\(y_1,\\dots,y_n\\) Probability Density for Observing \\(z_i\\) if true value is \\(y_i\\): \\[\\begin{equation} \\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp^{\\frac{(z_i-y_i)^2}{2\\sigma^2}} \\tag{9.5} \\end{equation}\\] Joint Density over all \\(n\\) terms: \\[\\begin{equation} \\begin{split} &amp; \\prod_{i=1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp^{\\frac{(z_i-y_i)^2}{2\\sigma^2}} \\\\ &amp;= \\bigg(\\frac{1}{\\sigma \\sqrt{2\\pi}}\\bigg)^n\\exp^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(z_i-y_i)^2} \\end{split} \\tag{9.6} \\end{equation}\\] Joint density is the product because the errors are all independent. Since the exponential function gets larger as exponent gets larger we can find the largest value of the whole expression by the largest exponent. We can remove the constants and we are left with a negative sum of the squared errors. If errors are normally distributed and independent and identically distributed (i.i.d.) then the set of parameters that minimizes the sum of squared errors is the maximum likelihood fit. Error ~ $N(0, ^2), i.i.d. Observations: \\(z_1,\\dots,z_n\\) Model estimates: \\(y_1,\\dots,y_n\\) MLE (Maximum Likelihood Example) The set of parameters that minimizes the sum of squared errors (explained above) Maximum Likelihood Fit: \\[\\begin{equation} \\max \\bigg(\\frac{1}{\\sigma \\sqrt{2\\pi}} \\bigg)^n\\exp^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(z_i-y_i)^2} \\rightarrow \\min \\sum_{i=1}^n (z_i-y_i)^2 \\tag{9.7} \\end{equation}\\] In the case of basic linear regression: \\(y_i = a_0 + \\sum_{j=1}^m a_j x_{ij}\\) \\(\\sum_{i=1}^n (z_i-y_i)^2\\) Minimize \\(\\sum_{i=1}^n (z_i-y_i)^2\\) over the parameters \\(a_0,\\dots,a_m\\) Maximum Likelihood of Basic Linear Regression: \\[\\begin{equation} \\min \\sum_{i=1}^n \\big(z_i - (a_0 + \\sum_{j=1}^m a_j x_{ij}) \\big)^2 \\tag{9.8} \\end{equation}\\] Can use likelihood to compare to different models with the likelihood ratio, the ratio of likelihoods and conducting a hypothesis test. 9.2.3 Maximum Likelihood Fitting Simple example: Regression with independent normally distributed errors Can get complex fast: Different estimation formulas Different assumptions about the error Good statistical software: Can handle more complexity than simple regression 9.2.4 Akaike Information Criterion (AIC) Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. Provide collection of models for the data AIC estimates quality of each model, relative to other models Provides a means for model selection Akaike Information Criterion (AIC): \\[\\begin{equation} AIC = 2k - 2\\ln(L^*) \\tag{9.9} \\end{equation}\\] \\(L^*\\): Maximum likelihood value \\(k\\): Number of parameters being estimated \\(2k\\) is a penalty term, balances likelihood with simplicity Helps avoid overfitting 9.3 AIC Example Regression with AIC: \\[\\begin{equation} \\begin{split} AIC &amp;= 2k - 2\\ln(L^*) \\\\ &amp;= 2k-2\\ln\\bigg(\\bigg(\\frac{1}{\\sigma \\sqrt{2\\pi}} \\bigg)^n\\exp^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\big(z_i- (a_0 + \\sum_{j=1}^m a_jx_{ij})\\big)^2}\\bigg) \\\\ &amp;= 2(m+1)-2\\ln\\bigg(\\bigg(\\frac{1}{\\sigma \\sqrt{2\\pi}} \\bigg)^n\\exp^{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\big(z_i- (a_0 + \\sum_{j=1}^m a_jx_{ij})\\big)^2}\\bigg) \\end{split} \\tag{9.10} \\end{equation}\\] Prefer models with smaller AIC Making AIC encourages fewer parameters \\(k\\) and higher likelihood Nice properties if there are infinitely many data points We aren’t going to have infinitely many data points and good luck storing it For smaller data sets we need a correction 9.3.1 Corrected AIC (\\(AIC_C\\)) Corrected AIC (\\(AIC_C\\)) is used for smaller data sets \\[\\begin{equation} \\begin{split} AIC_C &amp;= AIC + \\frac{2k(k+1)}{n-k-1} \\\\ &amp;= 2k - 2\\ln(L^*) + \\frac{2k(k+1)}{n-k-1} \\end{split} \\tag{9.11} \\end{equation}\\] \\(AIC_C\\) Example: Model 1: AIC = 75 Model 2: AIC = 80 Relative likelihood: \\[\\begin{equation} \\begin{split} &amp; \\exp^{\\frac{(AIC_1-AIC_2)}{2}} \\\\ &amp; \\exp^{\\frac{(75-80)}{2}} \\approx 8.2\\% \\end{split} \\tag{9.12} \\end{equation}\\] Model 2 is 8.2% as likely as Model 1 to be better This means the first model is probably better 9.3.2 Bayesian Information Criterion (BIC) When fitting models, it is possible to increase the maximum likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC for sample sizes greater than 7. Bayesian Information Criterion (BIC): \\[\\begin{equation} AIC = k\\ln(n) - 2\\ln(L^*) \\tag{9.13} \\end{equation}\\] \\(L^*\\): Maximum likelihood value \\(k\\): Number of parameters being estimated \\(n\\): Number of data points BIC is similar to AIC. BIC’s penalty term &gt; AIC’s penalty term BIC encourages models with fewer parameters than AIC does Only use BIC when there are more data points than parameters When comparing two models on the same data set by BIC their is a rule of thumb. BIC Rule of Thumb Comparing Models: \\(|BIC_1-BIC_2 &gt; 10\\) Smaller-BIC model is “very likely” better \\(6 &lt; |BIC_1-BIC_2 &lt; 10\\) Smaller-BIC model is “likely” better \\(2 &lt; |BIC_1-BIC_2 &lt; 6\\) Smaller-BIC model is “somewhat likely” better \\(0 &lt; |BIC_1-BIC_2 &lt; 2\\) Smaller-BIC model is “slightly likely” better 9.3.3 Summary No hard-and-fast rule for using AIC or BIC or maximum likelihood The AIC is a frequentist point-of-view and BIC is a bayesian point-of-view that get into the philosophy of statistics (beyond scope of course) All three can give valuable information Looking at all three can help you decide which is best Key Equations: Probability Density for Observing \\(z_i\\) if true value is \\(y_i\\): Eq. (9.5) Joint Density over all \\(n\\) terms: Eq. (9.6) Maximum Likelihood Fit: Eq. (9.7) Maximum Likelihood with Basic Linear Regression: Eq. (9.8) Akaike Information Criterion (AIC): Eq. (9.9) AIC with Regression: Eq. (9.10) Corrected AIC (\\(AIC_C\\)): Eq. (9.11) \\(AIC_C\\) Relative Likelihood: Eq. (9.12) Bayesian Information Criterion (BIC): Eq. (9.13) 9.4 M8L3 - Using Regression Lecture 3 - Using Regression (3:33) 9.5 M8L4 - Causation vs. Correlation This Lesson: Interpreting Regression Coefficients Lecture 4 - Causation vs. Correlation (6:10) 9.6 M8L5 - Transformations and Interactions Lecture 5 - Transformations and Interactions (2:17) 9.7 M8L6 - Output Lecture 6 - Output (5:05) "],["module-9---data-preparation-and-principle-component-analysis-pca.html", "10 Module 9 - Data Preparation and Principle Component Analysis (PCA) 10.1 M9L1 - Box-Cox Transformations 10.2 M9L2 - Detrending 10.3 M9L3 - Intro to Principal Component Analysis 10.4 M9L4 - Using Principal Component Analysis 10.5 M9L5 - Eigenvalues and Eigenvectors 10.6 M9L6 - PCA: The Good and Bad", " 10 Module 9 - Data Preparation and Principle Component Analysis (PCA) Module 9 covers advanced data preparation techniques, including principal component analysis. 10.1 M9L1 - Box-Cox Transformations Lecture 1 - Box-Cox Transformations (3:46) Transform data before using it Box-Cox transformation 10.1.1 Normality Assumption Some models assume data is normally distributed Results have bias when assumption is wrong Figure 10.1: Normality Assumption 10.1.2 Dealing with Heteroscedasticity Box-Cox Transformation Logarithmic transformation Stretches out the smaller range to enlarge its variability Shrinks the larger range to reduce its variability \\(t(y) = (y^{\\lambda}-1)/\\lambda\\) \\(t(y)\\) can become close to normal distribution Software can do it for you Check whether you need the transformation (e.g., Q-Q plot) 10.1.3 Summary Box-Cox: George Box and David Cox Box-Cox: Useful for transforming a response to eliminate heteroscedasticity 10.2 M9L2 - Detrending Lecture 2 - Detrending (3:40) 10.2.1 Detrending Data Time series data: Trend: increase or decrease of data over time For example: Price of gold over time Figure 10.2: Detrending Data - Price of Gold Over Time Detrending: Response Predictors Factor-based model Regression, SVM, etc. 10.2.2 How to Detrend Factor-by-factor one-dimensional regression: \\(y = a_o + a_1x\\) For example - simple linear regression for gold prices Price = -45,600 + 23.2 × Year De-trended price = Actual price - (-45,600 + 23.2 x Year) 10.2.3 Summary Detrending Approach Simple Works well to remove trend effects for time series data Helpful in factor-based analysis 10.3 M9L3 - Intro to Principal Component Analysis Lecture 3 - Intro to Principal Component Analysis (4:43) Feature extraction Principal Component Analysis (PCA) For high-dimensional and correlated data 10.3.1 Motivational Example Dealing with a model with a lot of facts Which of them are the most important? Example: Does the past performance of any particular stocks indicate how well the overall market will perform in the next day? Possible data sources 6000+ securities Focus on days without major external events wars terrorist attacks election results natural disasters etc. 10.3.2 Motivational Example - Potential Issues 6000+ predictors require many years of daily data But underlying situation may have changed with time Can we reduce the amount of data needed? High correlation between some of the predictors Companies in the same sector tend to move together Address both issues via Principal Component Analysis (PCA) 10.3.3 Principal Component Analysis (PCA) PCA transforms data Removes correlations within the data Ranks coordinates by importance Concentrate on the first n principal components Reduces effect of randomness Earlier principal components are likely to have higher signal-to-noise ratio 10.3.4 Summary Principal Component Analysis (PCA) For high-dimensional and correlated data PCA attempts to remove these correlations, and rank coordinates by importance Future Math behind PCA 10.4 M9L4 - Using Principal Component Analysis Lecture 4 - Using Principal Component Analysis (4:44) 10.4.1 Principle Component Analysis (PCA) \\(X\\): Initial matrix of data \\(x_{ij}\\) is the jth data factor of point \\(i\\) Scale such that \\(\\frac{1}{m}\\sum_i x_{ij} = \\mu_j = 0\\) Find all of the eigenvectors of \\(X^T X\\) \\(V\\): Matrix of eigenvectors (sorted by eigenvalue) \\(V = [V_1, V_2, \\dots]\\), where \\(V_j\\) is the jth eigenvector of \\(X^T X\\) PCA - Linear Transformation First component is \\(XV_1\\), second is \\(XV_2\\), etc. kth new factor value for the ith data point: \\(t_{ik}=\\sum_{j=1}^m x_{ij} v_{jk}\\) 10.4.2 PCA - Linear Transformation PCA eliminates correlation between factors Want fewer variables in your model? Only include the first \\(n\\) principal components Our math assumed we were looking for a linear transformation Dealing with nonlinear functions Using Kernels Similar to SVM modeling 10.4.3 PCA - Regression Interpret the new model in terms of original factors Example (Regression): PCA finds new L factors {\\(t_{ik}\\)}, then regression finds coefficients \\(b_0, b_1, \\dots, b_L\\) \\[\\begin{equation} \\begin{split} y_i &amp;= b_0 + \\sum_{k=1}^L b_k t_ik \\\\ &amp;= b_0 + \\sum_{k=1}^L b_k [\\sum_{j=1}^m x_{ij} v_{jk}] \\\\ &amp;= b_0 + \\sum_{j=1}^m x_{ij} [\\sum_{k=1}^L b_k v_{jk}]\\\\ &amp;= b_0 + \\sum_{j=1}^m x_{ij} [a_j] \\end{split} \\tag{10.1} \\end{equation}\\] Implied regression coefficient for \\(x_j\\): \\[\\begin{equation} a_j = \\sum^L_{k=1} b_k v_{jk} \\tag{10.2} \\end{equation}\\] 10.4.4 Summary Principal Component Analysis (PCA) For high-dimensional and correlated data PCA attempts to remove these correlations, and rank coordinates by importance PCA can be explained over the original factor space 10.5 M9L5 - Eigenvalues and Eigenvectors Lecture 5 - Eigenvalues and Eigenvectors (2:09) 10.5.1 Eigenvalues and Eigenvectors A: Square Matrix \\(v\\): Vector such that \\(Av = \\lambda v\\) \\(v\\): Eigenvector of A \\(\\lambda\\): Eigenvalue of A \\(\\det(A-\\lambda I) = 0\\) Given \\(\\lambda\\), solve \\(Av = \\lambda v\\) to find corresponding eigenvalue \\(v\\) 10.5.2 Principle Component Analysis (PCA) Scaled matrix \\(X\\) of data \\(x_{ij} =\\) jth value for data point \\(i\\), after scaling Find eigenvectors \\(v_1 \\dots v_n\\) of \\((X^T X)\\) Find the principal components Multiply \\(X\\) by the eigenvectors \\(Xv_1, Xv_2, \\dots Xv_n\\) are the principal components Transformed set of orthogonal coordinate directions 10.5.3 Summary Finding eigenvalues and eigenvectors of a square matrix Application to Principal Component Analysis (PCA) 10.6 M9L6 - PCA: The Good and Bad Lecture 6 - PCA: The Good and Bad (3:11) 10.6.1 PCA Review Change of coordinates Create uncorrelated factors Sorted by amount of variability explained Allows the use of small(er) number of variables Usually pick the ones that explain the most variability 10.6.2 Example: PCA is Good Figure 10.3: Example: Good PCA 10.6.3 Example: PCA Can Also Be Bad Figure 10.4: Example: Bad PCA 10.6.4 Summary Sometimes PCA can be good Sometimes PCA can be bad Often it can be a helpful approach to try "],["module-10---advanced-regression.html", "11 Module 10 - Advanced Regression 11.1 M10L1 - Introduction to CART 11.2 M10L2 - Branching 11.3 M10L3 - Random Forests 11.4 M10L3A - Explainability/Interpretability 11.5 M10L4 - Logistic Regression 11.6 M10L5 - Confusion Matrices 11.7 M10L6 - Situationally-Driven Comparison 11.8 M10L7 - Advanced Topics in Regression", " 11 Module 10 - Advanced Regression Module 10 covers advanced topics in regression, including tree-based approaches that can be used more generally. 11.1 M10L1 - Introduction to CART Lecture 1 - Introduction to Cart (5:57) 11.1.1 Overview Previously: Building, Fitting, Evaluating, and Using Regression Models This Lesson: Using Trees Dividing a data set Different models for different subsets 11.1.2 Trees in Regression Classification Problems CART: Classification and Regression Trees Decision Making Decision Tree Single Regression Modeling: - Ex: Impact of a marketing email on recipient spending - Predictors - Demographics: age, sex, number of children, income, etc. - Purchasing Factors: avg. amount spent per month on web site - Binary Factor: Was the email received Figure 11.1: Regression for Money Spent 25 Years Old or Younger Regression for “25 years old or younger” \\[\\begin{equation} \\mathrm{Money \\, Spent} = \\\\ 50 + 13.75[\\mathrm{Number \\, of \\, Children}] + 0[\\mathrm{Income \\, over \\,} \\$30,000] + \\dots \\end{equation}\\] Regression for “Older than 25” \\[\\begin{equation} \\mathrm{Money \\, Spent} = \\\\ 32 + 28.13[\\mathrm{Number \\, of \\, Children}] + 7.13[\\mathrm{Income \\, over \\,} \\$30,000] + \\dots \\end{equation}\\] Figure 11.2: Trees in Regression with Leaves Fewer data points per node Lots of regressions Figure 11.3: Regression Tree 11.1.3 Regression Formula in Trees Full Regression: NOT FOR TREES \\(y = a_0 + a_1x_1 + a_2x_2 + \\dots + a_nx_n\\) Regression in Trees: \\(y = a_0\\) \\[\\begin{equation} a_0 = \\frac{\\sum_i \\mathrm{in \\, node \\,} y_i}{\\mathrm{Number \\, of \\, data \\, points \\, in \\, node}} \\end{equation}\\] Quick Derivation of Best Fit: \\(\\underset{a_0}{\\min} \\sum_i (y_i-a_o)^2\\) Set Derivative to Zero: \\(2 \\sum_i (y_i - a_0) = 0\\) \\(\\sum_i y_i = k a_0\\) \\(a_0 = \\frac{\\sum_i y_i}{k}\\) 11.1.4 Final Comments Other places to use trees Logistic regression models Fraction of node’s data points with “true” response Classification models Most common classification among node’s data points Decision models Ex: Each leaf is the decision to “Do I send a marketing email?” Common Questions: How do we choose what branches to put into a tree? When do we stop branching? Why is this method called a Regression “Tree”? 11.2 M10L2 - Branching Lecture 2 - Branching (3:42) 11.2.1 Overview Previously: What is tree-based analysis Why should we consider making different models for different data subsets This Lesson: A tree’s branches Specifying Stopping conditions 11.2.2 Branching 11.2.3 Branching Methods 11.2.4 Summary How to perform branching When should a branch be kept in the model 11.3 M10L3 - Random Forests Lecture 3 - Random Forests (3:58) 11.3.1 Overview Previously: What is tree-based analysis Why should we consider making different models for different data subsets This Lesson: Random Forest Method 11.3.2 Tree Branchind 11.3.3 Random Forests 11.3.4 Introducing Randomness 11.3.5 Random Forests 11.3.6 Summary 11.4 M10L3A - Explainability/Interpretability Lecture 3A - Explainability/Interpretability (5:36) 11.4.1 Overview 11.5 M10L4 - Logistic Regression Lecture 4 - Logistic Regression (6:30) 11.5.1 Overview 11.5.2 Overview 11.6 M10L5 - Confusion Matrices Lecture 5 - Confusion Matrices (5:05) 11.6.1 Overview 11.7 M10L6 - Situationally-Driven Comparison Lecture 6 - Situationally-Driven Comparison (3:46) 11.7.1 Overview 11.8 M10L7 - Advanced Topics in Regression Lecture 7 - Advanced Topics in Regression (5:02) 11.8.1 Overview "],["appendix-b-discussion-questions.html", "12 Appendix B: Discussion Questions 12.1 Discussion Question 1.1 12.2 Discussion Question 1.2 12.3 Discussion Question 1.3 12.4 Discussion Question 1.4 12.5 Discussion Question 2.1 12.6 Discussion Question 2.2 12.7 Discussion Question 2.3 12.8 Discussion Question 3.1 12.9 Discussion Question 3.2 12.10 Discussion Question 3.3 12.11 Discussion Question 3.4 12.12 Discussion Question 4.1 12.13 Discussion Question 4.2 12.14 Discussion Question 4.3 12.15 Discussion Question 5.1 12.16 Discussion Question 5.2 12.17 Discussion Question 5.3 12.18 Discussion Question 6.1 12.19 Discussion Question 6.2 12.20 Discussion Question 6.3 12.21 Discussion Question 7.1 12.22 Discussion Question 7.2 12.23 Discussion Question 7.3 12.24 Discussion Question 7.4 12.25 Discussion Question 8.1 12.26 Discussion Question 8.2", " 12 Appendix B: Discussion Questions 12.1 Discussion Question 1.1 Why do we prefer a larger margin in hard-classification SVM? (Among other reasons, think about the correctness and certainty of data). What about in a soft-classification setting: what’s the benefit of a bigger margin (possibly with more error in classifying your known data) vs. a smaller margin (possibly with less error in classifying your known data), and what’s the benefit in the opposite direction (smaller margin)? 12.2 Discussion Question 1.2 Give examples of some situations where would you suggest scaling (where there are set minimum and maximum possible values) and some where you would suggest standardization (where, as a normal distribution, there isn’t a set minimum and maximum possible value). [Note: the terms “scaling” and “standardization” are unfortunately not standardized themselves! Some people use “scaling” to refer to a normal distribution, and some people use “standardization” to refer to scaling on an interval. Unfortunately, there’s nothing we can do about that.] 12.3 Discussion Question 1.3 In what sort of situations would SVM be better than KNN (and why), and in what sort of situations would KNN be better than SVM (and why)? Please think about this yourself, and discuss your own thoughts – I don’t just want someone to do a Google (or DuckDuckGo, etc.) search for an answer! Some possible things to think about: Think about how data points might be clustered. What if one class of point is much more common than the other in the data you’re using to build the model? [Preview: this data is called the “training data”, as we’ll see soon.] What does it mean (for KNN and for SVM) if you see that the classification of your points is very mixed (lots of red scattered within blue, and lots of blue scattered within red)? What do you think the SVM/KNN implications are if part of what you want to get out of a model is how likely it is that the classification of a new data point is correct? 12.4 Discussion Question 1.4 Why is scaling helpful before using KNN? 12.5 Discussion Question 2.1 If several models of almost-equal predictive quality are evaluated on the same set of validation data in order to select the model that performs best, is it likely that the truly best-predictive model will be chosen? 12.6 Discussion Question 2.2 Suppose you’ve run a bunch of models on validation data and you’ve picked the one that does best. Then you use test data to estimate how good that model is. A. Why is it likely that the goodness estimate from the test data isn’t as good as the validation data suggests? B. What if you run all of the models on the test data, and a different one looks best; should you switch your decision about which model to use? 12.7 Discussion Question 2.3 What is the fundamental difference between classification and clustering? Give an example of when to use each. 12.8 Discussion Question 3.1 What do you think is the hardest type of outlier to identify, and why? 12.9 Discussion Question 3.2 Once you’ve determined that a point is an outlier, it can be important to try to determine whether the outlier is (1) bad/incorrect data, and should be thrown out; (2) good/correct data but not relevant to the question at and and can be thrown out; or (3) good/correct data that is relevant to the question and therefore should be retained. Give examples of all three types of data. 12.10 Discussion Question 3.3 When using CUSUM, when might you want to use higher or lower values of C? When might you want to use higher or lower values of T? 12.11 Discussion Question 3.4 Give examples of when you might use a change-detection model. 12.12 Discussion Question 4.1 Give examples of time-series data (including ones that you might’ve dealt with at work). 12.13 Discussion Question 4.2 When would it be important to predict the variance of something? Give specific examples. 12.14 Discussion Question 4.3 How far in advance is an exponential-smoothing model accurate for forecasting? Explain. 12.15 Discussion Question 5.1 Regression and SVM lines have the same sort of formula: \\[ y = a_0 + a_1x_1 + a_2x_2+\\dots \\] For which of these models is the line affected more by points close to the line, and for which is the line affected more by points farther from the line? As a followup, can you think of pairs of similar situations where in one case you’d care more about the points closer to the line, and in the other case you’d care more about points farther from the line? 12.16 Discussion Question 5.2 Consider two regression models: one designed to estimate something involving physical properties and one designed to estimate something involving human behavior. Which do you expect to have a higher R-squared? 12.17 Discussion Question 5.3 How might you go about checking to see if a predictor has a linear or nonlinear relationship with the response? 12.18 Discussion Question 6.1 Give some examples of when you might want to detrend data. 12.19 Discussion Question 6.2 Suppose you start with a dataset that has predictor variables \\(x_1, x_2, \\dots, x_m\\) and you use PCA to transform them to variables \\(z_1, z_2, \\dots, z_m\\). If you run a linear regression on the original \\(x\\)-variables to get a predictive model \\(y_x = a_0 + a_1x_1 + a_2x_2 + \\dots + a_mx_m\\) and you run another linear regression on the transformed variables to get another predictive model \\(y_z = b_0 + b_1z_1 + b_2z_2 + \\dots + b_mz_m\\), which, if either, linear regression model will have a hhigher \\(R^2\\) and why? 12.20 Discussion Question 6.3 Suppose, before building a model (use linear regression as an example), you use PCA to identify the first principal component of your data, and use just that one component in your model. Explain (1) why you have not reduced the number of original features you’re using in the model, (2) why you have reduced the dimension of the data you’re using in the model, and (3) why you might want use just one principal component. 12.21 Discussion Question 7.1 In practice, software doesn’t fit a full model at each node of the trees (it would take too long); instead, it might just take the average value of all the data points at the node.  In the case of a regression tree, how is that similar to using a regression with no predictor variables? 12.22 Discussion Question 7.2 Consider these examples of three types of questions: Classification: Will it rain tomorrow? (We might use SVM or KNN) Probability estimation: What’s the probability it will rain tomorrow? (We might use logistic regression) Amount estimation: How much rain will there be tomorrow (We might use linear regression) Give more examples of three related questions, one where we might use SVM, one where we might use logistic regression, and one where we might use linear regression. 12.23 Discussion Question 7.3 Just like in the previous question, consider these three types of questions:  Classification: Will it rain tomorrow? (We might use SVM or KNN) Probability estimation: What’s the probability it will rain tomorrow? (We might use logistic regression) Amount estimation: How much rain will there be tomorrow (We might use linear regression) Think about the response variable in the input data, and about the model output. In classification, the response for each input data point will be either “yes” or “no” (or, 0 or 1, or some other binary pair of values). The output of an SVM or KNN classification model will be the same: either “yes” or “no”. In amount estimation, the response for each input data point will be an amount of something. The output of a linear regression model will be the same sort of information. But for probability estimation, there’s a difference: the output of a logistic regression model will be a probability between 0 and 1, but the response for each input data point will be either “1” (it happened) or “0” (it didn’t happen). For example, the training/test/validation data for a model asking “what’s the probability it will rain?” will all be past days where it either did rain (1) or didn’t rain (0). What do you think the implications are of the input data and the output data being different types? [This is a harder question than the previous one; try to reason through together piece by piece.] 12.24 Discussion Question 7.4 There’s an inherent tradeoff between models that fit more closely to data and models that fit less closely to data. Think back to the lessons on validation, where we saw that every model fits to both real patterns in the data (which are the same in other data) and random patterns (which are different in other data). If your model fits more to the data, you’re getting stronger fits to the real patterns, but you’re also getting more fits to random patterns (“overfitting”). On the other hand, if your model fits less to the data, you’re getting less-strong fits to the real patterns (“underfitting”), but you’re also getting less fit to random patterns. Suppose you have a model that performs much worse on validation data than on training data. Suggest some ways you could address the problem (some might relate to the mini-summary above, and some might not). 12.25 Discussion Question 8.1 Which is more likely to be overfit, linear regression or ridge regression, and why? 12.26 Discussion Question 8.2 In what sort of situation(s) is variable selection necessary, instead of just using all available variables? "],["appendix-b-glossary.html", "13 Appendix B: Glossary 13.1 Basic Machine Learning", " 13 Appendix B: Glossary 13.1 Basic Machine Learning Lessons 2.1-2.2, 2.4-2.6, 2.8, 4.1, 4.3-4.6, 6.1-6.3, 16.4 Algorithm: Step-by-step procedure designed to carry out a task. Change detection: Identifying when a significant change has taken place in a process. Classification: The separation of data into two or more categories, or (a point’s classification) the category a data point is put into. Classifier: A boundary that separates the data into two or more categories. Also (more generally) an algorithm that performs classification. Cluster: A group of points identified as near/similar to each other. Cluster center: In some clustering algorithms (like 𝑘-means clustering), the central point (often the centroid) of a cluster of data points. Clustering: Separation of data points into groups (“clusters”) based on nearness/similarity to each other. A common form of unsupervised learning. CUSUM: Change detection method that compares observed distribution mean with a threshold level of change. Short for “cumulative sum”. Deep learning: Neural network-type model with many hidden layers. Dimension: A feature of the data points (for example, height or credit score). (Note that there is also a mathematical definition for this word.) EM algorithm: Expectation-maximization algorithm. Expectation-maximization algorithm (EM algorithm): General description of an algorithm with two steps (often iterated), one that finds the function for the expected likelihood of getting the response given current parameters, and one that finds new parameter values to maximize that probability. Heuristic: Algorithm that is not guaranteed to find the absolute best (optimal) solution. k-means algorithm: Clustering algorithm that defines 𝑘 clusters of data points, each corresponding to one of 𝑘 cluster centers selected by the algorithm. k-Nearest-Neighbor (KNN): Classification algorithm that defines a data point’s category as a function of the nearest 𝑘 data points to it. Kernel: A type of function that computes the similarity between two inputs; thanks to what’s (really!) sometimes known as the “kernel trick”, nonlinear classifiers can be found almost as easily as linear ones. Learning: Finding/discovering patterns (or rules) in data, often that can be applied to new data. Machine: Apparatus that can do something; in “machine learning”, it often refers to both an algorithm and the computer it’s run on. (Fun fact: before computers were developed, the term “computers” referred to people who did calculations quickly in their heads or on paper!) Margin: For a single point, the distance between the point and the classification boundary; for a set of points, the minimum distance between a point in the set and the classification boundary. Also called the separation. Machine learning: Use of computer algorithms to learn and discover patterns or structure in data, without being programmed specifically for them. Misclassified: Put into the wrong category by a classifier. Neural network: A machine learning model that itself is modeled after the workings of neurons in the brain. Supervised learning: Machine learning where the “correct” answer is known for each data point in the training set. Support vector: In SVM models, the closest point to the classifier, among those in a category. (Note that there is a more-technical mathematical definition too.) Support vector machine (SVM): Classification algorithm that uses a boundary to separate the data into two or more categories (“classes”). SVM: Support vector machine. Unsupervised learning: Machine learning where the “correct” answer is not known for the data points in the training set. Voronoi diagram: Graphical representation of splitting a plane with two or more special points into regions with one special point each, where each region’s points are closest to that special point. [1] S. Arlot and A. Celisse, “A survey of cross-validation procedures for model selection,” 2010. [2] Wikibooks, “Data mining algorithms in r/clustering/k-means.” http://en.wikipedia.org/w/index.php?title=K-means%20clustering&amp;oldid=1243054475, 2024. "]]
