[["module-10---advanced-regression.html", "10 Module 10 - Advanced Regression 10.1 M10L1 - Introduction to CART 10.2 M10L2 - Branching 10.3 M10L3 - Random Forests 10.4 M10L3A - Explainability/Interpretability 10.5 M10L4 - Logistic Regression 10.6 M10L5 - Confusion Matrices 10.7 M10L6 - Situationally-Driven Comparison 10.8 M10L7 - Advanced Topics in Regression", " 10 Module 10 - Advanced Regression Module 10 covers advanced topics in regression, including tree-based approaches that can be used more generally. 10.1 M10L1 - Introduction to CART Lecture 1 - Introduction to Cart (5:57) 10.1.1 Overview Previously: Building, Fitting, Evaluating, and Using Regression Models This Lesson: Using Trees Dividing a data set Different models for different subsets 10.1.2 Trees in Regression Classification Problems CART: Classification and Regression Trees Decision Making Decision Tree Single Regression Modeling: - Ex: Impact of a marketing email on recipient spending - Predictors - Demographics: age, sex, number of children, income, etc. - Purchasing Factors: avg. amount spent per month on web site - Binary Factor: Was the email received Figure 10.1: Regression for Money Spent 25 Years Old or Younger Regression for “25 years old or younger” \\[\\begin{equation} \\mathrm{Money \\, Spent} = \\\\ 50 + 13.75[\\mathrm{Number \\, of \\, Children}] + 0[\\mathrm{Income \\, over \\,} \\$30,000] + \\dots \\end{equation}\\] Regression for “Older than 25” \\[\\begin{equation} \\mathrm{Money \\, Spent} = \\\\ 32 + 28.13[\\mathrm{Number \\, of \\, Children}] + 7.13[\\mathrm{Income \\, over \\,} \\$30,000] + \\dots \\end{equation}\\] Figure 10.2: Trees in Regression with Leaves Fewer data points per node Lots of regressions Figure 10.3: Regression Tree 10.1.3 Regression Formula in Trees Full Regression: NOT FOR TREES \\(y = a_0 + a_1x_1 + a_2x_2 + \\dots + a_nx_n\\) Regression in Trees: \\(y = a_0\\) \\[\\begin{equation} a_0 = \\frac{\\sum_i \\mathrm{in \\, node \\,} y_i}{\\mathrm{Number \\, of \\, data \\, points \\, in \\, node}} \\end{equation}\\] Quick Derivation of Best Fit: \\(\\underset{a_0}{\\min} \\sum_i (y_i-a_o)^2\\) Set Derivative to Zero: \\(2 \\sum_i (y_i - a_0) = 0\\) \\(\\sum_i y_i = k a_0\\) \\(a_0 = \\frac{\\sum_i y_i}{k}\\) 10.1.4 Final Comments Other places to use trees Logistic regression models Fraction of node’s data points with “true” response Classification models Most common classification among node’s data points Decision models Ex: Each leaf is the decision to “Do I send a marketing email?” Common Questions: How do we choose what branches to put into a tree? When do we stop branching? Why is this method called a Regression “Tree”? 10.2 M10L2 - Branching Lecture 2 - Branching (3:42) 10.2.1 Overview Previously: What is tree-based analysis Why should we consider making different models for different data subsets This Lesson: A tree’s branches Specifying Stopping conditions 10.2.2 Branching 10.2.3 Branching Methods 10.2.4 Summary How to perform branching When should a branch be kept in the model 10.3 M10L3 - Random Forests Lecture 3 - Random Forests (3:58) 10.3.1 Overview Previously: What is tree-based analysis Why should we consider making different models for different data subsets This Lesson: Random Forest Method 10.3.2 Tree Branchind 10.3.3 Random Forests 10.3.4 Introducing Randomness 10.3.5 Random Forests 10.3.6 Summary 10.4 M10L3A - Explainability/Interpretability Lecture 3A - Explainability/Interpretability (5:36) 10.4.1 Overview 10.5 M10L4 - Logistic Regression Lecture 4 - Logistic Regression (6:30) 10.5.1 Overview 10.5.2 Overview 10.6 M10L5 - Confusion Matrices Lecture 5 - Confusion Matrices (5:05) 10.6.1 Overview 10.7 M10L6 - Situationally-Driven Comparison Lecture 6 - Situationally-Driven Comparison (3:46) 10.7.1 Overview 10.8 M10L7 - Advanced Topics in Regression Lecture 7 - Advanced Topics in Regression (5:02) 10.8.1 Overview "]]
