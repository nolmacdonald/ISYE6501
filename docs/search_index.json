[["module-9---data-preparation-and-principle-component-analysis-pca.html", "9 Module 9 - Data Preparation and Principle Component Analysis (PCA) 9.1 M9L1 - Box-Cox Transformations 9.2 M9L2 - Detrending 9.3 M9L3 - Intro to Principal Component Analysis 9.4 M9L4 - Using Principal Component Analysis 9.5 M9L5 - Eigenvalues and Eigenvectors 9.6 M9L6 - PCA: The Good and Bad", " 9 Module 9 - Data Preparation and Principle Component Analysis (PCA) Module 9 covers advanced data preparation techniques, including principal component analysis. 9.1 M9L1 - Box-Cox Transformations Lecture 1 - Box-Cox Transformations (3:46) Transform data before using it Box-Cox transformation 9.1.1 Normality Assumption Some models assume data is normally distributed Results have bias when assumption is wrong Figure 9.1: Normality Assumption 9.1.2 Dealing with Heteroscedasticity Box-Cox Transformation Logarithmic transformation Stretches out the smaller range to enlarge its variability Shrinks the larger range to reduce its variability \\(t(y) = (y^{\\lambda}-1)/\\lambda\\) \\(t(y)\\) can become close to normal distribution Software can do it for you Check whether you need the transformation (e.g., Q-Q plot) 9.1.3 Summary Box-Cox: George Box and David Cox Box-Cox: Useful for transforming a response to eliminate heteroscedasticity 9.2 M9L2 - Detrending Lecture 2 - Detrending (3:40) 9.2.1 Detrending Data Time series data: Trend: increase or decrease of data over time For example: Price of gold over time Figure 9.2: Detrending Data - Price of Gold Over Time Detrending: Response Predictors Factor-based model Regression, SVM, etc. 9.2.2 How to Detrend Factor-by-factor one-dimensional regression: \\(y = a_o + a_1x\\) For example - simple linear regression for gold prices Price = -45,600 + 23.2 Ã— Year De-trended price = Actual price - (-45,600 + 23.2 x Year) 9.2.3 Summary Detrending Approach Simple Works well to remove trend effects for time series data Helpful in factor-based analysis 9.3 M9L3 - Intro to Principal Component Analysis Lecture 3 - Intro to Principal Component Analysis (4:43) Feature extraction Principal Component Analysis (PCA) For high-dimensional and correlated data 9.3.1 Motivational Example Dealing with a model with a lot of facts Which of them are the most important? Example: Does the past performance of any particular stocks indicate how well the overall market will perform in the next day? Possible data sources 6000+ securities Focus on days without major external events wars terrorist attacks election results natural disasters etc. 9.3.2 Motivational Example - Potential Issues 6000+ predictors require many years of daily data But underlying situation may have changed with time Can we reduce the amount of data needed? High correlation between some of the predictors Companies in the same sector tend to move together Address both issues via Principal Component Analysis (PCA) 9.3.3 Principal Component Analysis (PCA) PCA transforms data Removes correlations within the data Ranks coordinates by importance Concentrate on the first n principal components Reduces effect of randomness Earlier principal components are likely to have higher signal-to-noise ratio 9.3.4 Summary Principal Component Analysis (PCA) For high-dimensional and correlated data PCA attempts to remove these correlations, and rank coordinates by importance Future Math behind PCA 9.4 M9L4 - Using Principal Component Analysis Lecture 4 - Using Principal Component Analysis (4:44) 9.4.1 Principle Component Analysis (PCA) \\(X\\): Initial matrix of data \\(x_{ij}\\) is the jth data factor of point \\(i\\) Scale such that \\(\\frac{1}{m}\\sum_i x_{ij} = \\mu_j = 0\\) Find all of the eigenvectors of \\(X^T X\\) \\(V\\): Matrix of eigenvectors (sorted by eigenvalue) \\(V = [V_1, V_2, \\dots]\\), where \\(V_j\\) is the jth eigenvector of \\(X^T X\\) PCA - Linear Transformation First component is \\(XV_1\\), second is \\(XV_2\\), etc. kth new factor value for the ith data point: \\(t_{ik}=\\sum_{j=1}^m x_{ij} v_{jk}\\) 9.4.2 PCA - Linear Transformation PCA eliminates correlation between factors Want fewer variables in your model? Only include the first \\(n\\) principal components Our math assumed we were looking for a linear transformation Dealing with nonlinear functions Using Kernels Similar to SVM modeling 9.4.3 PCA - Regression Interpret the new model in terms of original factors Example (Regression): PCA finds new L factors {\\(t_{ik}\\)}, then regression finds coefficients \\(b_0, b_1, \\dots, b_L\\) \\[\\begin{equation} \\begin{split} y_i &amp;= b_0 + \\sum_{k=1}^L b_k t_ik \\\\ &amp;= b_0 + \\sum_{k=1}^L b_k [\\sum_{j=1}^m x_{ij} v_{jk}] \\\\ &amp;= b_0 + \\sum_{j=1}^m x_{ij} [\\sum_{k=1}^L b_k v_{jk}]\\\\ &amp;= b_0 + \\sum_{j=1}^m x_{ij} [a_j] \\end{split} \\tag{9.1} \\end{equation}\\] Implied regression coefficient for \\(x_j\\): \\[\\begin{equation} a_j = \\sum^L_{k=1} b_k v_{jk} \\tag{9.2} \\end{equation}\\] 9.4.4 Summary Principal Component Analysis (PCA) For high-dimensional and correlated data PCA attempts to remove these correlations, and rank coordinates by importance PCA can be explained over the original factor space 9.5 M9L5 - Eigenvalues and Eigenvectors Lecture 5 - Eigenvalues and Eigenvectors (2:09) 9.5.1 Eigenvalues and Eigenvectors A: Square Matrix \\(v\\): Vector such that \\(Av = \\lambda v\\) \\(v\\): Eigenvector of A \\(\\lambda\\): Eigenvalue of A \\(\\det(A-\\lambda I) = 0\\) Given \\(\\lambda\\), solve \\(Av = \\lambda v\\) to find corresponding eigenvalue \\(v\\) 9.5.2 Principle Component Analysis (PCA) Scaled matrix \\(X\\) of data \\(x_{ij} =\\) jth value for data point \\(i\\), after scaling Find eigenvectors \\(v_1 \\dots v_n\\) of \\((X^T X)\\) Find the principal components Multiply \\(X\\) by the eigenvectors \\(Xv_1, Xv_2, \\dots Xv_n\\) are the principal components Transformed set of orthogonal coordinate directions 9.5.3 Summary Finding eigenvalues and eigenvectors of a square matrix Application to Principal Component Analysis (PCA) 9.6 M9L6 - PCA: The Good and Bad Lecture 6 - PCA: The Good and Bad (3:11) 9.6.1 PCA Review Change of coordinates Create uncorrelated factors Sorted by amount of variability explained Allows the use of small(er) number of variables Usually pick the ones that explain the most variability 9.6.2 Example: PCA is Good Figure 9.3: Example: Good PCA 9.6.3 Example: PCA Can Also Be Bad Figure 9.4: Example: Bad PCA 9.6.4 Summary Sometimes PCA can be good Sometimes PCA can be bad Often it can be a helpful approach to try "]]
